"""
Generator: selects model, formats prompt, calls model APIs, returns content and references
Supports: Claude (placeholder), Gemini (placeholder), OpenAI
"""
from src.config import settings
from src.generation.prompt_templates import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
from src.utils.logger import logger
from typing import Dict, List

def select_model(preferred=None):
    preferred = preferred or settings.PRIMARY_MODEL
    # Map friendly names -> internal provider calls
    # TODO: add real SDK calls for Claude & Gemini
    if "claude" in preferred.lower():
        return "claude"
    if "gemini" in preferred.lower():
        return "gemini"
    return "openai"

def build_prompt(content_request: Dict, context_docs: List[Dict]) -> str:
    context = "\n\n".join([f"[source:{d.get('id')}] {d.get('text')[:1000]}" for d in context_docs])
    system = SYSTEM_PROMPT.format(style="professional", tone=content_request.get("tone","neutral"), audience=content_request.get("target_audience","general"))
    user = USER_PROMPT_TEMPLATE.format(
        type=content_request["type"],
        topic=content_request["topic"],
        word_count=content_request.get("word_count",600),
        target_audience=content_request.get("target_audience","general"),
        context=context,
        tone=content_request.get("tone","professional_conversational"),
        keywords=", ".join(content_request.get("seo_keywords", []))
    )
    return system + "\n\n" + user

def generate_content(content_request: Dict, context_docs: List[Dict], generation_config: Dict) -> Dict:
    model_choice = select_model(generation_config.get("model"))
    prompt = build_prompt(content_request, context_docs)
    # Simple OpenAI fallback call
    if model_choice == "openai":
        import openai
        openai.api_key = settings.OPENAI_API_KEY
        resp = openai.ChatCompletion.create(
            model="gpt-4o" if "gpt" in generation_config.get("model","").lower() else "gpt-4o",
            messages=[{"role":"system","content":prompt}],
            temperature=generation_config.get("temperature",0.7),
            max_tokens= generation_config.get("max_tokens", 1500)
        )
        text = resp["choices"][0]["message"]["content"]
    else:
        # TODO: implement real Claude / Gemini SDK calls
        logger.warning(f"Model {model_choice} not implemented fully; returning placeholder.")
        text = f"[Generated by {model_choice}] {prompt[:500]}"
    return {
        "generated_text": text,
        "model": generation_config.get("model"),
        "metadata": {"used_docs": [d["id"] for d in context_docs]}
    }
