"""
Generator: selects model, formats prompt, calls model APIs, returns content and references
Supports: Claude (placeholder), Gemini (placeholder), OpenAI
"""
from src.config import settings
from src.generation.prompt_templates import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
from src.utils.logger import logger
from typing import Dict, List
from anthropic import Anthropic
import google.generativeai as genai
from openai import OpenAI
from src.config import settings   # wherever you keep your env vars
from typing import Dict, List
from src.config import settings
# from src.model_selector import select_model
from src.generation.prompt_templates import build_prompt
from loguru import logger
from anthropic import Anthropic
import google.generativeai as genai
import openai

def select_model(preferred=None):
    """
    Returns a ready-to-call model client based on the preferred provider.
    """
    preferred = (preferred or settings.PRIMARY_MODEL).lower()

    if "claude" in preferred:
        # Claude (Anthropic)
        client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        # Youâ€™d call with: client.messages.create(model="claude-3-opus-20240229", messages=[...])
        return client

    if "gemini" in preferred:
        # Gemini (Google Generative AI)
        genai.configure(api_key=settings.GEMINI_API_KEY)
        # Example usage later: genai.GenerativeModel("gemini-1.5-pro")
        return genai

    # Default to OpenAI
    return OpenAI(api_key=settings.OPENAI_API_KEY)


def build_prompt(content_request: Dict, context_docs: List[Dict]) -> str:
    context = "\n\n".join([f"[source:{d.get('id')}] {d.get('text')[:1000]}" for d in context_docs])
    system = SYSTEM_PROMPT.format(style="professional", tone=content_request.get("tone","neutral"), audience=content_request.get("target_audience","general"))
    user = USER_PROMPT_TEMPLATE.format(
        type=content_request["type"],
        topic=content_request["topic"],
        word_count=content_request.get("word_count",600),
        target_audience=content_request.get("target_audience","general"),
        context=context,
        tone=content_request.get("tone","professional_conversational"),
        keywords=", ".join(content_request.get("seo_keywords", []))
    )
    return system + "\n\n" + user


def generate_content(content_request: Dict, context_docs: List[Dict], generation_config: Dict) -> Dict:
    """
    Generate text using OpenAI, Claude, or Gemini depending on generation_config["model"].
    """
    model_choice = select_model(generation_config.get("model"))
    prompt = build_prompt(content_request, context_docs)

    # === OpenAI ===
    if isinstance(model_choice, openai.OpenAI) or model_choice == "openai":
        openai.api_key = settings.OPENAI_API_KEY
        resp = openai.ChatCompletion.create(
            model=generation_config.get("model", "gpt-4o"),
            messages=[{"role": "system", "content": prompt}],
            temperature=generation_config.get("temperature", 0.7),
            max_tokens=generation_config.get("max_tokens", 1500)
        )
        text = resp["choices"][0]["message"]["content"]

    # === Claude (Anthropic) ===
    elif isinstance(model_choice, Anthropic):
        client: Anthropic = model_choice
        resp = client.messages.create(
            model=generation_config.get("model", "claude-3-opus-20240229"),
            max_tokens=generation_config.get("max_tokens", 1500),
            temperature=generation_config.get("temperature", 0.7),
            messages=[{"role": "user", "content": prompt}]
        )
        text = resp.content[0].text

    # === Gemini ===
    elif hasattr(model_choice, "GenerativeModel"):   # google.generativeai module
        genai.configure(api_key=settings.GEMINI_API_KEY)
        gmodel = genai.GenerativeModel(generation_config.get("model", "gemini-1.5-pro"))
        resp = gmodel.generate_content(prompt,
                                       generation_config={"temperature": generation_config.get("temperature", 0.7),
                                                          "max_output_tokens": generation_config.get("max_tokens", 1500)})
        text = resp.text

    else:
        logger.warning(f"Model {model_choice} not implemented fully; returning placeholder.")
        text = f"[Generated by {model_choice}] {prompt[:500]}"

    return {
        "generated_text": text,
        "model": generation_config.get("model"),
        "metadata": {"used_docs": [d["id"] for d in context_docs]}
    }

