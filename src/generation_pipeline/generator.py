"""
Generator: selects model, formats prompt, calls model APIs, returns content and references
Supports: Claude (placeholder), Gemini (placeholder), OpenAI
"""
from src.config import settings
from src.generation.prompt_templates import SYSTEM_PROMPT, USER_PROMPT_TEMPLATE
from src.utils.logger import logger
from typing import Dict, List
from anthropic import Anthropic
import google.generativeai as genai
from openai import OpenAI
from src.config import settings   # wherever you keep your env vars
from typing import Dict, List
from src.config import settings
# from src.model_selector import select_model
from src.generation.prompt_templates import build_prompt
from loguru import logger
from anthropic import Anthropic
import google.generativeai as genai
import openai
from src.generation.prompt_templates import build_prompt

def select_model(preferred=None):
    """
    Returns a ready-to-call model client based on the preferred provider.
    """
    preferred = (preferred or settings.PRIMARY_MODEL).lower()

    if "claude" in preferred:
        # Claude (Anthropic)
        client = Anthropic(api_key=settings.ANTHROPIC_API_KEY)
        # You’d call with: client.messages.create(model="claude-3-opus-20240229", messages=[...])
        return client

    if "gemini" in preferred:
        # Gemini (Google Generative AI)
        genai.configure(api_key=settings.GEMINI_API_KEY)
        # Example usage later: genai.GenerativeModel("gemini-1.5-pro")
        return genai

    # Default to OpenAI
    return OpenAI(api_key=settings.OPENAI_API_KEY)

from huggingface_hub import InferenceClient

def generate_content(content_request, context_docs, generation_config):
    # …whatever prep code you already have…

    # build your full prompt
    prompt = f"{content_request}\n\nContext:\n{context_docs}"

    # create HF client (do this once at the top of the file ideally)
    client = InferenceClient(
        "tiiuae/falcon-7b-instruct",
        token="hf_your_token_here"
    )

    # call the model
    resp = client.text_generation(
        prompt,
        max_new_tokens=400,
        temperature=generation_config.get("temperature", 0.7)
    )

    return resp

def generate_content(content_request: Dict, context_docs: List[Dict], generation_config: Dict) -> Dict:
    """
    Generate text using OpenAI, Claude, or Gemini depending on generation_config["model"].
    """
    model_choice = select_model(generation_config.get("model"))
    prompt = build_prompt(content_request, context_docs)
    
    messages = [
        {"role": "system", "content": "You are a helpful AI content generator."},
        {"role": "user", "content": prompt}
    ]
    # === OpenAI ===
    if isinstance(model_choice, openai.OpenAI) or model_choice == "openai":
        openai.api_key = settings.OPENAI_API_KEY

        resp = openai.chat.completions.create(
            model=generation_config.get("model", "gpt-4"),
            messages=messages,
            temperature=generation_config.get("temperature", 0.7),
            max_tokens=generation_config.get("max_tokens", 600)
        )

        generated_text = resp.choices[0].message.content
        text = resp["choices"][0]["message"]["content"]

    # === Claude (Anthropic) ===
    elif isinstance(model_choice, Anthropic):
        client: Anthropic = model_choice
        resp = client.messages.create(
            model=generation_config.get("model", "claude-3-opus-20240229"),
            max_tokens=generation_config.get("max_tokens", 1500),
            temperature=generation_config.get("temperature", 0.7),
            messages=[{"role": "user", "content": prompt}]
        )
        text = resp.content[0].text

    elif isinstance(model_choice, str) and model_choice.startswith("gemini"):
        import google.generativeai as genai
        genai.configure(api_key=settings.GEMINI_API_KEY)

        gmodel = genai.GenerativeModel(
        generation_config.get("model", "gemini-2.5-pro")
    )

        resp = gmodel.generate_content(
            prompt,
            generation_config={
                "temperature": generation_config.get("temperature", 0.7),
                "max_output_tokens": generation_config.get("max_tokens", 1500),
            },
        )
        text = resp.text


    else:
        logger.warning(f"Model {model_choice} not implemented fully; returning placeholder.")
        text = f"[Generated by {model_choice}] {prompt[:500]}"

    return {
        "generated_text": text,
        "model": generation_config.get("model"),
        "metadata": {"used_docs": [d["id"] for d in context_docs]}
    }

